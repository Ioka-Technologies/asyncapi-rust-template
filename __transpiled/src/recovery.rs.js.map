{"version":3,"file":"recovery.rs.js","sources":["../../template/src/recovery.rs.js"],"sourcesContent":["export default function RecoveryRs() {\n    return (\n        <File name=\"recovery.rs\">\n            {`//! Error recovery and resilience patterns for AsyncAPI operations\n//!\n//! This module provides:\n//! - Retry mechanisms with exponential backoff\n//! - Circuit breaker pattern for preventing cascade failures\n//! - Bulkhead pattern for failure isolation\n//! - Dead letter queue handling\n//! - Graceful degradation strategies\n\nuse crate::errors::{AsyncApiError, AsyncApiResult, ErrorMetadata, ErrorSeverity, ErrorCategory};\nuse std::time::{Duration, Instant};\nuse std::sync::Arc;\nuse tokio::sync::RwLock;\nuse tracing::{info, warn, error, debug};\nuse serde::{Deserialize, Serialize};\n\n/// Retry configuration for different operation types\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RetryConfig {\n    /// Maximum number of retry attempts\n    pub max_attempts: u32,\n    /// Initial delay between retries\n    pub initial_delay: Duration,\n    /// Maximum delay between retries\n    pub max_delay: Duration,\n    /// Backoff multiplier for exponential backoff\n    pub backoff_multiplier: f64,\n    /// Maximum total time to spend retrying\n    pub max_total_time: Duration,\n    /// Jitter factor to add randomness to delays (0.0 to 1.0)\n    pub jitter_factor: f64,\n}\n\nimpl Default for RetryConfig {\n    fn default() -> Self {\n        Self {\n            max_attempts: 3,\n            initial_delay: Duration::from_millis(100),\n            max_delay: Duration::from_secs(30),\n            backoff_multiplier: 2.0,\n            max_total_time: Duration::from_secs(300), // 5 minutes\n            jitter_factor: 0.1,\n        }\n    }\n}\n\nimpl RetryConfig {\n    /// Create a conservative retry config for critical operations\n    pub fn conservative() -> Self {\n        Self {\n            max_attempts: 5,\n            initial_delay: Duration::from_millis(500),\n            max_delay: Duration::from_secs(60),\n            backoff_multiplier: 1.5,\n            max_total_time: Duration::from_secs(600), // 10 minutes\n            jitter_factor: 0.2,\n        }\n    }\n\n    /// Create an aggressive retry config for non-critical operations\n    pub fn aggressive() -> Self {\n        Self {\n            max_attempts: 10,\n            initial_delay: Duration::from_millis(50),\n            max_delay: Duration::from_secs(10),\n            backoff_multiplier: 2.5,\n            max_total_time: Duration::from_secs(120), // 2 minutes\n            jitter_factor: 0.05,\n        }\n    }\n\n    /// Create a fast retry config for real-time operations\n    pub fn fast() -> Self {\n        Self {\n            max_attempts: 2,\n            initial_delay: Duration::from_millis(10),\n            max_delay: Duration::from_millis(500),\n            backoff_multiplier: 2.0,\n            max_total_time: Duration::from_secs(5),\n            jitter_factor: 0.1,\n        }\n    }\n}\n\n/// Retry strategy implementation with exponential backoff and jitter\npub struct RetryStrategy {\n    config: RetryConfig,\n    start_time: Instant,\n    attempt: u32,\n}\n\nimpl RetryStrategy {\n    pub fn new(config: RetryConfig) -> Self {\n        Self {\n            config,\n            start_time: Instant::now(),\n            attempt: 0,\n        }\n    }\n\n    /// Execute an operation with retry logic\n    pub async fn execute<F, Fut, T>(&mut self, operation: F) -> AsyncApiResult<T>\n    where\n        F: Fn() -> Fut,\n        Fut: std::future::Future<Output = AsyncApiResult<T>>,\n    {\n        loop {\n            self.attempt += 1;\n\n            debug!(\n                attempt = self.attempt,\n                max_attempts = self.config.max_attempts,\n                \"Executing operation with retry\"\n            );\n\n            match operation().await {\n                Ok(result) => {\n                    if self.attempt > 1 {\n                        info!(\n                            attempt = self.attempt,\n                            elapsed = ?self.start_time.elapsed(),\n                            \"Operation succeeded after retry\"\n                        );\n                    }\n                    return Ok(result);\n                }\n                Err(error) => {\n                    // Check if we should retry\n                    if !self.should_retry(&error) {\n                        warn!(\n                            attempt = self.attempt,\n                            error = %error,\n                            \"Operation failed with non-retryable error\"\n                        );\n                        return Err(error);\n                    }\n\n                    // Check if we've exceeded retry limits\n                    if self.attempt >= self.config.max_attempts {\n                        error!(\n                            attempt = self.attempt,\n                            max_attempts = self.config.max_attempts,\n                            \"Maximum retry attempts exceeded\"\n                        );\n                        return Err(AsyncApiError::Recovery {\n                            message: format!(\n                                \"Operation failed after {} attempts: {}\",\n                                self.attempt, error\n                            ),\n                            attempts: self.attempt,\n                            metadata: ErrorMetadata::new(\n                                ErrorSeverity::High,\n                                ErrorCategory::Resource,\n                                false,\n                            ),\n                            source: Some(Box::new(error)),\n                        });\n                    }\n\n                    // Check total time limit\n                    if self.start_time.elapsed() >= self.config.max_total_time {\n                        error!(\n                            elapsed = ?self.start_time.elapsed(),\n                            max_total_time = ?self.config.max_total_time,\n                            \"Maximum retry time exceeded\"\n                        );\n                        return Err(AsyncApiError::Recovery {\n                            message: format!(\n                                \"Operation failed within time limit: {}\",\n                                error\n                            ),\n                            attempts: self.attempt,\n                            metadata: ErrorMetadata::new(\n                                ErrorSeverity::High,\n                                ErrorCategory::Resource,\n                                false,\n                            ),\n                            source: Some(Box::new(error)),\n                        });\n                    }\n\n                    // Calculate delay and wait\n                    let delay = self.calculate_delay();\n                    warn!(\n                        attempt = self.attempt,\n                        delay_ms = delay.as_millis(),\n                        error = %error,\n                        \"Operation failed, retrying after delay\"\n                    );\n\n                    tokio::time::sleep(delay).await;\n                }\n            }\n        }\n    }\n\n    fn should_retry(&self, error: &AsyncApiError) -> bool {\n        // Don't retry non-retryable errors\n        if !error.is_retryable() {\n            return false;\n        }\n\n        // Don't retry validation or security errors\n        match error.category() {\n            ErrorCategory::Validation | ErrorCategory::Security => false,\n            _ => true,\n        }\n    }\n\n    fn calculate_delay(&self) -> Duration {\n        let base_delay = self.config.initial_delay.as_millis() as f64\n            * self.config.backoff_multiplier.powi((self.attempt - 1) as i32);\n\n        let max_delay = self.config.max_delay.as_millis() as f64;\n        let delay = base_delay.min(max_delay);\n\n        // Add jitter to prevent thundering herd\n        let jitter = delay * self.config.jitter_factor * (rand::random::<f64>() - 0.5);\n        let final_delay = (delay + jitter).max(0.0) as u64;\n\n        Duration::from_millis(final_delay)\n    }\n\n    /// Get current attempt number\n    pub fn current_attempt(&self) -> u32 {\n        self.attempt\n    }\n}\n\n/// Circuit breaker states\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum CircuitBreakerState {\n    /// Circuit is closed, requests are allowed\n    Closed,\n    /// Circuit is open, requests are rejected\n    Open,\n    /// Circuit is half-open, testing if service has recovered\n    HalfOpen,\n}\n\n/// Circuit breaker configuration\n#[derive(Debug, Clone)]\npub struct CircuitBreakerConfig {\n    /// Number of failures before opening the circuit\n    pub failure_threshold: u32,\n    /// Time to wait before transitioning from Open to HalfOpen\n    pub recovery_timeout: Duration,\n    /// Number of successful requests needed to close the circuit from HalfOpen\n    pub success_threshold: u32,\n    /// Time window for counting failures\n    pub failure_window: Duration,\n}\n\nimpl Default for CircuitBreakerConfig {\n    fn default() -> Self {\n        Self {\n            failure_threshold: 5,\n            recovery_timeout: Duration::from_secs(60),\n            success_threshold: 3,\n            failure_window: Duration::from_secs(60),\n        }\n    }\n}\n\n/// Circuit breaker implementation for preventing cascade failures\n#[derive(Debug)]\npub struct CircuitBreaker {\n    config: CircuitBreakerConfig,\n    state: Arc<RwLock<CircuitBreakerState>>,\n    failure_count: Arc<RwLock<u32>>,\n    success_count: Arc<RwLock<u32>>,\n    last_failure_time: Arc<RwLock<Option<Instant>>>,\n    last_state_change: Arc<RwLock<Instant>>,\n}\n\nimpl CircuitBreaker {\n    pub fn new(config: CircuitBreakerConfig) -> Self {\n        Self {\n            config,\n            state: Arc::new(RwLock::new(CircuitBreakerState::Closed)),\n            failure_count: Arc::new(RwLock::new(0)),\n            success_count: Arc::new(RwLock::new(0)),\n            last_failure_time: Arc::new(RwLock::new(None)),\n            last_state_change: Arc::new(RwLock::new(Instant::now())),\n        }\n    }\n\n    /// Execute an operation through the circuit breaker\n    pub async fn execute<F, Fut, T>(&self, operation: F) -> AsyncApiResult<T>\n    where\n        F: Fn() -> Fut,\n        Fut: std::future::Future<Output = AsyncApiResult<T>>,\n    {\n        // Check if circuit should transition states\n        self.check_state_transition().await;\n\n        let current_state = *self.state.read().await;\n\n        match current_state {\n            CircuitBreakerState::Open => {\n                debug!(\"Circuit breaker is open, rejecting request\");\n                Err(AsyncApiError::Resource {\n                    message: \"Circuit breaker is open\".to_string(),\n                    resource_type: \"circuit_breaker\".to_string(),\n                    metadata: ErrorMetadata::new(\n                        ErrorSeverity::Medium,\n                        ErrorCategory::Resource,\n                        true,\n                    ),\n                    source: None,\n                })\n            }\n            CircuitBreakerState::Closed | CircuitBreakerState::HalfOpen => {\n                match operation().await {\n                    Ok(result) => {\n                        self.record_success().await;\n                        Ok(result)\n                    }\n                    Err(error) => {\n                        self.record_failure().await;\n                        Err(error)\n                    }\n                }\n            }\n        }\n    }\n\n    async fn record_success(&self) {\n        let mut success_count = self.success_count.write().await;\n        *success_count += 1;\n\n        let current_state = *self.state.read().await;\n        if current_state == CircuitBreakerState::HalfOpen\n            && *success_count >= self.config.success_threshold {\n            info!(\"Circuit breaker transitioning to Closed state\");\n            *self.state.write().await = CircuitBreakerState::Closed;\n            *self.failure_count.write().await = 0;\n            *success_count = 0;\n            *self.last_state_change.write().await = Instant::now();\n        }\n    }\n\n    async fn record_failure(&self) {\n        let mut failure_count = self.failure_count.write().await;\n        *failure_count += 1;\n        *self.last_failure_time.write().await = Some(Instant::now());\n\n        let current_state = *self.state.read().await;\n        if current_state == CircuitBreakerState::Closed\n            && *failure_count >= self.config.failure_threshold {\n            warn!(\n                failure_count = *failure_count,\n                threshold = self.config.failure_threshold,\n                \"Circuit breaker transitioning to Open state\"\n            );\n            *self.state.write().await = CircuitBreakerState::Open;\n            *self.success_count.write().await = 0;\n            *self.last_state_change.write().await = Instant::now();\n        } else if current_state == CircuitBreakerState::HalfOpen {\n            warn!(\"Circuit breaker transitioning back to Open state\");\n            *self.state.write().await = CircuitBreakerState::Open;\n            *self.success_count.write().await = 0;\n            *self.last_state_change.write().await = Instant::now();\n        }\n    }\n\n    async fn check_state_transition(&self) {\n        let current_state = *self.state.read().await;\n        let last_change = *self.last_state_change.read().await;\n\n        if current_state == CircuitBreakerState::Open\n            && last_change.elapsed() >= self.config.recovery_timeout {\n            info!(\"Circuit breaker transitioning to HalfOpen state\");\n            *self.state.write().await = CircuitBreakerState::HalfOpen;\n            *self.last_state_change.write().await = Instant::now();\n        }\n\n        // Reset failure count if outside failure window\n        if let Some(last_failure) = *self.last_failure_time.read().await {\n            if last_failure.elapsed() >= self.config.failure_window {\n                *self.failure_count.write().await = 0;\n            }\n        }\n    }\n\n    /// Get current circuit breaker state\n    pub async fn state(&self) -> CircuitBreakerState {\n        *self.state.read().await\n    }\n\n    /// Get current failure count\n    pub async fn failure_count(&self) -> u32 {\n        *self.failure_count.read().await\n    }\n}\n\n/// Dead letter queue for handling unprocessable messages\n#[derive(Debug)]\npub struct DeadLetterQueue {\n    max_size: usize,\n    messages: Arc<RwLock<Vec<DeadLetterMessage>>>,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DeadLetterMessage {\n    pub id: String,\n    pub original_channel: String,\n    pub payload: Vec<u8>,\n    pub error: String,\n    pub timestamp: chrono::DateTime<chrono::Utc>,\n    pub retry_count: u32,\n}\n\nimpl DeadLetterQueue {\n    pub fn new(max_size: usize) -> Self {\n        Self {\n            max_size,\n            messages: Arc::new(RwLock::new(Vec::new())),\n        }\n    }\n\n    /// Add a message to the dead letter queue\n    pub async fn add_message(\n        &self,\n        channel: &str,\n        payload: Vec<u8>,\n        error: &AsyncApiError,\n        retry_count: u32,\n    ) -> AsyncApiResult<()> {\n        let mut messages = self.messages.write().await;\n\n        // Remove oldest message if at capacity\n        if messages.len() >= self.max_size {\n            messages.remove(0);\n            warn!(\"Dead letter queue at capacity, removing oldest message\");\n        }\n\n        let message = DeadLetterMessage {\n            id: uuid::Uuid::new_v4().to_string(),\n            original_channel: channel.to_string(),\n            payload,\n            error: error.to_string(),\n            timestamp: chrono::Utc::now(),\n            retry_count,\n        };\n\n        messages.push(message);\n        info!(\n            channel = channel,\n            error = %error,\n            queue_size = messages.len(),\n            \"Message added to dead letter queue\"\n        );\n\n        Ok(())\n    }\n\n    /// Get all messages in the dead letter queue\n    pub async fn get_messages(&self) -> Vec<DeadLetterMessage> {\n        self.messages.read().await.clone()\n    }\n\n    /// Remove a message from the dead letter queue\n    pub async fn remove_message(&self, message_id: &str) -> bool {\n        let mut messages = self.messages.write().await;\n        if let Some(pos) = messages.iter().position(|m| m.id == message_id) {\n            messages.remove(pos);\n            true\n        } else {\n            false\n        }\n    }\n\n    /// Clear all messages from the dead letter queue\n    pub async fn clear(&self) {\n        let mut messages = self.messages.write().await;\n        let count = messages.len();\n        messages.clear();\n        info!(cleared_count = count, \"Dead letter queue cleared\");\n    }\n\n    /// Get queue size\n    pub async fn size(&self) -> usize {\n        self.messages.read().await.len()\n    }\n}\n\n/// Bulkhead pattern for isolating failures\n#[derive(Debug)]\npub struct Bulkhead {\n    name: String,\n    semaphore: Arc<tokio::sync::Semaphore>,\n    max_concurrent: usize,\n    timeout: Duration,\n}\n\nimpl Bulkhead {\n    pub fn new(name: String, max_concurrent: usize, timeout: Duration) -> Self {\n        Self {\n            name,\n            semaphore: Arc::new(tokio::sync::Semaphore::new(max_concurrent)),\n            max_concurrent,\n            timeout,\n        }\n    }\n\n    /// Execute an operation within the bulkhead\n    pub async fn execute<F, Fut, T>(&self, operation: F) -> AsyncApiResult<T>\n    where\n        F: Fn() -> Fut,\n        Fut: std::future::Future<Output = AsyncApiResult<T>>,\n    {\n        // Try to acquire permit with timeout\n        let permit = match tokio::time::timeout(\n            self.timeout,\n            self.semaphore.acquire()\n        ).await {\n            Ok(Ok(permit)) => permit,\n            Ok(Err(_)) => {\n                return Err(AsyncApiError::Resource {\n                    message: format!(\"Bulkhead '{}' semaphore closed\", self.name),\n                    resource_type: \"bulkhead\".to_string(),\n                    metadata: ErrorMetadata::new(\n                        ErrorSeverity::High,\n                        ErrorCategory::Resource,\n                        true,\n                    ),\n                    source: None,\n                });\n            }\n            Err(_) => {\n                return Err(AsyncApiError::Resource {\n                    message: format!(\n                        \"Bulkhead '{}' timeout waiting for permit (max_concurrent: {})\",\n                        self.name, self.max_concurrent\n                    ),\n                    resource_type: \"bulkhead\".to_string(),\n                    metadata: ErrorMetadata::new(\n                        ErrorSeverity::Medium,\n                        ErrorCategory::Resource,\n                        true,\n                    ),\n                    source: None,\n                });\n            }\n        };\n\n        debug!(\n            bulkhead = %self.name,\n            available_permits = self.semaphore.available_permits(),\n            \"Executing operation within bulkhead\"\n        );\n\n        // Execute operation with permit held\n        let result = operation().await;\n\n        // Permit is automatically released when dropped\n        drop(permit);\n\n        result\n    }\n\n    /// Get current available permits\n    pub fn available_permits(&self) -> usize {\n        self.semaphore.available_permits()\n    }\n}\n\n/// Recovery manager that coordinates all recovery strategies\n#[derive(Debug)]\npub struct RecoveryManager {\n    retry_configs: std::collections::HashMap<String, RetryConfig>,\n    circuit_breakers: std::collections::HashMap<String, Arc<CircuitBreaker>>,\n    dead_letter_queue: Arc<DeadLetterQueue>,\n    bulkheads: std::collections::HashMap<String, Arc<Bulkhead>>,\n}\n\nimpl RecoveryManager {\n    pub fn new() -> Self {\n        Self {\n            retry_configs: std::collections::HashMap::new(),\n            circuit_breakers: std::collections::HashMap::new(),\n            dead_letter_queue: Arc::new(DeadLetterQueue::new(1000)),\n            bulkheads: std::collections::HashMap::new(),\n        }\n    }\n\n    /// Configure retry strategy for an operation type\n    pub fn configure_retry(&mut self, operation_type: &str, config: RetryConfig) {\n        self.retry_configs.insert(operation_type.to_string(), config);\n    }\n\n    /// Configure circuit breaker for a service\n    pub fn configure_circuit_breaker(&mut self, service: &str, config: CircuitBreakerConfig) {\n        let circuit_breaker = Arc::new(CircuitBreaker::new(config));\n        self.circuit_breakers.insert(service.to_string(), circuit_breaker);\n    }\n\n    /// Configure bulkhead for a resource\n    pub fn configure_bulkhead(&mut self, resource: &str, max_concurrent: usize, timeout: Duration) {\n        let bulkhead = Arc::new(Bulkhead::new(resource.to_string(), max_concurrent, timeout));\n        self.bulkheads.insert(resource.to_string(), bulkhead);\n    }\n\n    /// Get retry strategy for operation type\n    pub fn get_retry_strategy(&self, operation_type: &str) -> RetryStrategy {\n        let config = self.retry_configs\n            .get(operation_type)\n            .cloned()\n            .unwrap_or_default();\n        RetryStrategy::new(config)\n    }\n\n    /// Get circuit breaker for service\n    pub fn get_circuit_breaker(&self, service: &str) -> Option<Arc<CircuitBreaker>> {\n        self.circuit_breakers.get(service).cloned()\n    }\n\n    /// Get dead letter queue\n    pub fn get_dead_letter_queue(&self) -> Arc<DeadLetterQueue> {\n        self.dead_letter_queue.clone()\n    }\n\n    /// Get bulkhead for resource\n    pub fn get_bulkhead(&self, resource: &str) -> Option<Arc<Bulkhead>> {\n        self.bulkheads.get(resource).cloned()\n    }\n}\n\nimpl Default for RecoveryManager {\n    fn default() -> Self {\n        let mut manager = Self::new();\n\n        // Configure default retry strategies\n        manager.configure_retry(\"message_handler\", RetryConfig::default());\n        manager.configure_retry(\"connection\", RetryConfig::conservative());\n        manager.configure_retry(\"validation\", RetryConfig::fast());\n\n        // Configure default circuit breakers\n        manager.configure_circuit_breaker(\"default\", CircuitBreakerConfig::default());\n\n        // Configure default bulkheads\n        manager.configure_bulkhead(\"message_processing\", 100, Duration::from_secs(30));\n        manager.configure_bulkhead(\"connection_pool\", 50, Duration::from_secs(10));\n\n        manager\n    }\n}\n`}\n        </File>\n    );\n}\n"],"names":["RecoveryRs","_jsx","File","name","children"],"mappings":";;;;;AAAe,SAASA,UAAUA,GAAG;EACjC,oBACIC,cAAA,CAACC,IAAI,EAAA;AAACC,IAAAA,IAAI,EAAC,aAAa;AAAAC,IAAAA,QAAA,EACnB,CAAA;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAA;AAAC,GACa,CAAC,CAAA;AAEf;;;;"}